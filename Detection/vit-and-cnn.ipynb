{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1379553,"sourceType":"datasetVersion","datasetId":804753}],"dockerImageVersionId":30236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U tensorflow_addons","metadata":{"id":"PJTOExN4ZWiY","outputId":"75a9a7d0-d8f1-49fe-a187-2c50efbfe2aa","execution":{"iopub.status.busy":"2024-10-07T08:57:55.098938Z","iopub.execute_input":"2024-10-07T08:57:55.099375Z","iopub.status.idle":"2024-10-07T08:58:18.45505Z","shell.execute_reply.started":"2024-10-07T08:57:55.09929Z","shell.execute_reply":"2024-10-07T08:58:18.453989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Common\nimport keras\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import image as tfi\n\n# Data\nfrom keras.datasets import cifar100\n\n# Data Viz\nimport matplotlib.pyplot as plt\n\n# Data Aug\nfrom keras.layers import Normalization\nfrom keras.layers import Resizing\nfrom keras.layers import RandomFlip\nfrom keras.layers import RandomRotation\nfrom keras.layers import RandomZoom\n\n\n# Model\nfrom tensorflow.nn import gelu\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.layers import Layer\nfrom keras.layers import Input\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers import MultiHeadAttention\nfrom keras.layers import LayerNormalization\nfrom keras.layers import Add\nfrom keras.layers import Flatten\n\n# Compiling\nfrom keras.losses import SparseCategoricalCrossentropy as SCCe\nfrom tensorflow_addons.optimizers import AdamW\nfrom keras.metrics import SparseCategoricalAccuracy as Acc\nfrom keras.metrics import SparseTopKCategoricalAccuracy as KAcc\n\n# Callbacks \nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping","metadata":{"id":"2QuZSxaB-c8j","execution":{"iopub.status.busy":"2024-10-07T08:58:18.457651Z","iopub.execute_input":"2024-10-07T08:58:18.458067Z","iopub.status.idle":"2024-10-07T08:58:23.425054Z","shell.execute_reply.started":"2024-10-07T08:58:18.458026Z","shell.execute_reply":"2024-10-07T08:58:23.423967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Functions**","metadata":{"id":"kZ8cp7lJD7Xf"}},{"cell_type":"code","source":"def show_image(image, title=None, cmap=None):\n    plt.imshow(image, cmap=cmap)\n    if title is not None:\n        plt.title(title)\n    plt.axis('off')","metadata":{"id":"iGKEpUsWD7OC","execution":{"iopub.status.busy":"2024-10-07T08:58:23.426412Z","iopub.execute_input":"2024-10-07T08:58:23.427173Z","iopub.status.idle":"2024-10-07T08:58:23.432707Z","shell.execute_reply.started":"2024-10-07T08:58:23.427127Z","shell.execute_reply":"2024-10-07T08:58:23.431594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Data**","metadata":{"id":"yUrY9nvjDX7N"}},{"cell_type":"code","source":"batch_size = 100\nimg_height = 32\nimg_width = 32","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:23.435664Z","iopub.execute_input":"2024-10-07T08:58:23.435964Z","iopub.status.idle":"2024-10-07T08:58:23.454334Z","shell.execute_reply.started":"2024-10-07T08:58:23.435935Z","shell.execute_reply":"2024-10-07T08:58:23.453337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_train, y_train), (X_valid, y_valid) = cifar100.load_data()\n\ntraining_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/train',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size\n\n)\n\ntesting_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/test',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)\n\nvalidation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/val',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)","metadata":{"id":"g005KYZDDXVv","execution":{"iopub.status.busy":"2024-10-07T08:58:23.455524Z","iopub.execute_input":"2024-10-07T08:58:23.455783Z","iopub.status.idle":"2024-10-07T08:58:32.368804Z","shell.execute_reply.started":"2024-10-07T08:58:23.455759Z","shell.execute_reply":"2024-10-07T08:58:32.367759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Parameters**","metadata":{"id":"yERCLBaAJwH1"}},{"cell_type":"code","source":"SIZE = 72\nPATCH_SIZE = 6\nLR = 0.001\nWEIGHT_DECAY = 0.0001\nEPOCHS = 100\nNUM_PATCHES = (SIZE // PATCH_SIZE) ** 2\nPROJECTION_DIMS = 64\nNUM_HEADS = 4\nHIDDEN_UNITS = [PROJECTION_DIMS*2, PROJECTION_DIMS]\nOUTPUT_UNITS = [2048,1024]","metadata":{"id":"wjxVmNoZJvrC","execution":{"iopub.status.busy":"2024-10-07T08:58:32.370564Z","iopub.execute_input":"2024-10-07T08:58:32.371378Z","iopub.status.idle":"2024-10-07T08:58:32.377816Z","shell.execute_reply.started":"2024-10-07T08:58:32.371328Z","shell.execute_reply":"2024-10-07T08:58:32.376676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. **Data Augmentation**","metadata":{"id":"dbXXvgZ1FT2P"}},{"cell_type":"code","source":"class DataAugmentation(Layer):\n    def __init__(self, norm, SIZE):\n        super(DataAugmentation, self).__init__()\n        self.norm = norm\n        self.SIZE = SIZE\n        self.resize = Resizing(SIZE, SIZE)\n        self.flip = RandomFlip('horizontal')\n        self.rotation = RandomRotation(factor=0.02)\n        self.zoom = RandomZoom(height_factor=0.2, width_factor=0.2)\n    def call(self, X):\n        x = self.norm(X)\n        x = self.resize(x)\n        x = self.flip(x)\n        x = self.rotation(x)\n        x = self.zoom(x)\n        return x\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n        \"norm\": self.norm,\n        \"SIZE\": self.SIZE,\n        })\n        return config","metadata":{"id":"IkrRo6OrE2p7","execution":{"iopub.status.busy":"2024-10-07T08:58:32.379166Z","iopub.execute_input":"2024-10-07T08:58:32.379462Z","iopub.status.idle":"2024-10-07T08:58:32.390798Z","shell.execute_reply.started":"2024-10-07T08:58:32.379435Z","shell.execute_reply":"2024-10-07T08:58:32.389799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. **Image-to-Patches**","metadata":{"id":"RASOwC_MHOcX"}},{"cell_type":"code","source":"class Patches(Layer):\n\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0] # Get the Batch Size\n        patches = tfi.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1], # only along the Height and Width Dimension\n            strides=[1, self.patch_size, self.patch_size, 1], # The next patch should not overlap the previus patch\n            rates=[1,1,1,1],\n            padding='VALID'\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"path-size\": self.patch_size,\n        })\n        return config","metadata":{"id":"K-zOcL-mG0mX","execution":{"iopub.status.busy":"2024-10-07T08:58:32.392231Z","iopub.execute_input":"2024-10-07T08:58:32.392615Z","iopub.status.idle":"2024-10-07T08:58:32.404059Z","shell.execute_reply.started":"2024-10-07T08:58:32.392578Z","shell.execute_reply":"2024-10-07T08:58:32.403011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how his works ","metadata":{"id":"Yygsbv0SIpa6"}},{"cell_type":"code","source":"image = X_train[np.random.randint(len(X_train))]\nimage = tfi.resize(image, (SIZE, SIZE))\nimage = tf.cast(image,'uint8')\nshow_image(image)\nimage = tf.expand_dims(image,0)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:32.40549Z","iopub.execute_input":"2024-10-07T08:58:32.405935Z","iopub.status.idle":"2024-10-07T08:58:32.601514Z","shell.execute_reply.started":"2024-10-07T08:58:32.405897Z","shell.execute_reply":"2024-10-07T08:58:32.600546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patch = Patches(PATCH_SIZE) # patch Size\npatches = patch(image)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:32.605067Z","iopub.execute_input":"2024-10-07T08:58:32.60539Z","iopub.status.idle":"2024-10-07T08:58:32.653684Z","shell.execute_reply.started":"2024-10-07T08:58:32.605354Z","shell.execute_reply":"2024-10-07T08:58:32.650208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patches.shape # We needed 12 patches","metadata":{"id":"XmSuOfPeI_1K","outputId":"43558a37-9c55-4b68-d4fe-3c9f9fceee2e","execution":{"iopub.status.busy":"2024-10-07T08:58:32.656991Z","iopub.execute_input":"2024-10-07T08:58:32.657837Z","iopub.status.idle":"2024-10-07T08:58:32.666368Z","shell.execute_reply.started":"2024-10-07T08:58:32.657762Z","shell.execute_reply":"2024-10-07T08:58:32.665229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nshow_image(image[0], title=\"Original Image\")\nplt.show()\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(5,5))\nplt.suptitle(\"Patches\", fontsize=15)\nfor i in range(patches.shape[-2]):\n    plt.subplot(n,n,i+1)\n    patch = tf.reshape(patches[0][i],(PATCH_SIZE, PATCH_SIZE, 3))\n    patch = tf.cast(patch, 'uint8')\n    show_image(patch, cmap='gray')\nplt.show()","metadata":{"id":"8SB27W_QKXMF","outputId":"8aaf524d-e2bf-401e-8ed6-978b62fb6f0f","execution":{"iopub.status.busy":"2024-10-07T08:58:32.668104Z","iopub.execute_input":"2024-10-07T08:58:32.671525Z","iopub.status.idle":"2024-10-07T08:58:37.770018Z","shell.execute_reply.started":"2024-10-07T08:58:32.671485Z","shell.execute_reply":"2024-10-07T08:58:37.768985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. **MLP**\n---\nPatches MLP","metadata":{"id":"TP5JZHp-Mm8_"}},{"cell_type":"code","source":"class PatchEncoder(Layer):\n\n    def __init__(self, num_patches, projection_dims): # Projection dims is  D\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.d = projection_dims\n\n        self.dense = Dense(units=projection_dims)\n        self.positional_embeddings = Embedding(input_dim=num_patches, output_dim=projection_dims)\n\n    def call(self, X):\n        positions = tf.range(0,limit=self.num_patches, delta=1)\n        encoded = self.dense(X) + self.positional_embeddings(positions)\n        return encoded\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"num_paches\": self.num_patches,\n            \"d\": self.d,\n        })\n        return config","metadata":{"id":"dyCHj38SMmvf","execution":{"iopub.status.busy":"2024-10-07T08:58:37.771468Z","iopub.execute_input":"2024-10-07T08:58:37.772404Z","iopub.status.idle":"2024-10-07T08:58:37.781061Z","shell.execute_reply.started":"2024-10-07T08:58:37.772352Z","shell.execute_reply":"2024-10-07T08:58:37.779918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. **MLP**\n---\nOutput MLP","metadata":{"id":"JwE4BvZCOBsc"}},{"cell_type":"code","source":"class MLP(Layer):\n    def __init__(self, units, rate):\n        super(MLP, self).__init__()\n        self.units = units\n        self.rate = rate\n        self.layers = [[Dense(unit, activation=gelu), Dropout(rate)] for unit in units]\n\n    def call(self, x):\n        for layers in self.layers:\n          for layer in layers:\n            x = layer(x)\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"units\": self.units,\n            \"rate\": self.rate,\n        })\n        return config","metadata":{"id":"rKLvpC9TK-i4","execution":{"iopub.status.busy":"2024-10-07T08:58:37.782372Z","iopub.execute_input":"2024-10-07T08:58:37.782654Z","iopub.status.idle":"2024-10-07T08:58:38.573857Z","shell.execute_reply.started":"2024-10-07T08:58:37.782628Z","shell.execute_reply":"2024-10-07T08:58:38.572721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Transformet Network**","metadata":{"id":"2Q9fsg5BRYeg"}},{"cell_type":"code","source":"class Transformer(Layer):\n\n    def __init__(self, L, num_heads, key_dims, hidden_units):\n        super(Transformer, self).__init__()\n        self.L = L\n        self.heads = num_heads\n        self.key_dims = key_dims\n        self.hidden_units = hidden_units\n\n        self.norm = LayerNormalization(epsilon=1e-6) # Remember the Params\n        self.MHA = MultiHeadAttention(num_heads=num_heads, key_dim=key_dims, dropout=0.1)\n        self.net = MLP(units=hidden_units, rate=0.1)\n        self.add= Add()\n\n    def call(self, X):\n        inputs = X\n        x = X\n        for _ in range(self.L):\n          x = self.norm(x)\n          x = self.MHA(x,x) # our Target and the Source element are the same\n          y = self.add([x,inputs])\n          x = self.norm(y)\n          x = self.net(x)\n          x = self.add([x,y])\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"L\": self.L,\n            \"heads\": self.heads,\n            \"key_dims\":self.key_dims,\n            \"hidden_units\":self.hidden_units\n        })\n        return config","metadata":{"id":"1hQ0Kwe5RXk7","execution":{"iopub.status.busy":"2024-10-07T08:58:38.575072Z","iopub.execute_input":"2024-10-07T08:58:38.575408Z","iopub.status.idle":"2024-10-07T08:58:38.587025Z","shell.execute_reply.started":"2024-10-07T08:58:38.575379Z","shell.execute_reply":"2024-10-07T08:58:38.586043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **ViT Structure**","metadata":{"id":"4yXoNnlaPuJd"}},{"cell_type":"code","source":"class_names = training_ds.class_names","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:38.588375Z","iopub.execute_input":"2024-10-07T08:58:38.588761Z","iopub.status.idle":"2024-10-07T08:58:38.602024Z","shell.execute_reply.started":"2024-10-07T08:58:38.588723Z","shell.execute_reply":"2024-10-07T08:58:38.601119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\nprint(f'Input Image Shape : {input_shape}')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:38.603442Z","iopub.execute_input":"2024-10-07T08:58:38.603819Z","iopub.status.idle":"2024-10-07T08:58:38.614433Z","shell.execute_reply.started":"2024-10-07T08:58:38.603782Z","shell.execute_reply":"2024-10-07T08:58:38.613263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input Layer\ninputs = Input(shape=input_shape)\n\n# Apply Data Aug\nnorm = Normalization()\nnorm.adapt(X_train)\n\nx = DataAugmentation(norm, SIZE)(inputs)\n\n# Get Patches\nx = Patches(PATCH_SIZE)(x)\n\n# PatchEncoding Network\nx = PatchEncoder(NUM_PATCHES, PROJECTION_DIMS)(x)\n\n# Transformer Network\nx = Transformer(8, NUM_HEADS, PROJECTION_DIMS, HIDDEN_UNITS)(x)\n\n# Output Network\nx = LayerNormalization(epsilon=1e-6)(x)\nx = Flatten()(x)\nx = Dropout(0.5)(x)\n\nx = MLP(OUTPUT_UNITS, rate=0.5)(x)\n\n# Ouput Layer\noutputs = Dense(100)(x)","metadata":{"id":"UhUdg3xSPfmB","execution":{"iopub.status.busy":"2024-10-07T08:58:38.615961Z","iopub.execute_input":"2024-10-07T08:58:38.616742Z","iopub.status.idle":"2024-10-07T08:58:42.163743Z","shell.execute_reply.started":"2024-10-07T08:58:38.616675Z","shell.execute_reply":"2024-10-07T08:58:42.162782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **ViT Model**","metadata":{"id":"moPnaDDBW_q8"}},{"cell_type":"code","source":"with tf.device('/GPU:0'):\n  # Model\n  model = Model(\n    inputs=[inputs],\n    outputs=[outputs],\n  )\n\n  # Compiling\n  model.compile(\n      loss=SCCe(from_logits=True),\n      optimizer=AdamW(learning_rate=LR, weight_decay=WEIGHT_DECAY),\n      metrics=[\n          Acc(name=\"Accuracy\"),\n          KAcc(5, name=\"Top-5-Accuracy\")\n      ]\n  )\n\n  # Callbacks\n  cbs = [\n    ModelCheckpoint(\"ViT-Model.h5\", save_best_only=True)\n  ]\n\n  ","metadata":{"id":"gktmx2vpQ92a","outputId":"a5dbcce0-8776-4065-a30d-5a9b6aaf0e62","execution":{"iopub.status.busy":"2024-10-07T08:58:42.164944Z","iopub.execute_input":"2024-10-07T08:58:42.165273Z","iopub.status.idle":"2024-10-07T08:58:42.193595Z","shell.execute_reply.started":"2024-10-07T08:58:42.165244Z","shell.execute_reply":"2024-10-07T08:58:42.19256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit\nresults = model.fit(\n      training_ds,\n      epochs=EPOCHS,\n      validation_data=validation_ds,\n      callbacks=cbs\n  )","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:58:42.194825Z","iopub.execute_input":"2024-10-07T08:58:42.19515Z","iopub.status.idle":"2024-10-07T09:03:46.008598Z","shell.execute_reply.started":"2024-10-07T08:58:42.195115Z","shell.execute_reply":"2024-10-07T09:03:46.007665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('mymodel.keras')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:46.009977Z","iopub.execute_input":"2024-10-07T09:03:46.010336Z","iopub.status.idle":"2024-10-07T09:03:46.552984Z","shell.execute_reply.started":"2024-10-07T09:03:46.010304Z","shell.execute_reply":"2024-10-07T09:03:46.552148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viewing Results","metadata":{}},{"cell_type":"code","source":"AccuracyVector = []\nplt.figure(figsize=(30, 30))\nfor images, labels in testing_ds.take(1):\n    predictions = model.predict(images)\n    predlabel = []\n    prdlbl = []\n    \n    for mem in predictions:\n        predlabel.append(class_names[np.argmax(mem)])\n        prdlbl.append(np.argmax(mem))\n    \n    AccuracyVector = np.array(prdlbl) == labels\n    for i in range(40):\n        ax = plt.subplot(10, 6, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title('Pred: '+ predlabel[i]+' actl:'+class_names[labels[i]] )\n        plt.axis('off')\n        plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:46.554225Z","iopub.execute_input":"2024-10-07T09:03:46.554507Z","iopub.status.idle":"2024-10-07T09:03:49.968463Z","shell.execute_reply.started":"2024-10-07T09:03:46.554482Z","shell.execute_reply":"2024-10-07T09:03:49.967365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"truePositive=0\ntrueNegative=0\nfalsePositive=0\nfalseNegative=0\n#positive event is accident negative event is non accident\nfor i in range(0,100):\n    if(predlabel[i]==class_names[labels[i]] and predlabel[i]=='Accident'):\n        truePositive+=1\n    elif(predlabel[i]==class_names[labels[i]] and predlabel[i]=='Non Accident'):\n        trueNegative+=1\n    elif(predlabel[i]=='Non Accident' and class_names[labels[i]]=='Accident'):\n        falseNegative+=1\n    else:\n        falsePositive+=1","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:49.969898Z","iopub.execute_input":"2024-10-07T09:03:49.970293Z","iopub.status.idle":"2024-10-07T09:03:50.005331Z","shell.execute_reply.started":"2024-10-07T09:03:49.970263Z","shell.execute_reply":"2024-10-07T09:03:50.004313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'True positives are: {truePositive}')\nprint(f'True negatives are: {trueNegative}')\nprint(f'False negatives are: {falseNegative}')\nprint(f'False positives are: {falsePositive}')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:50.006702Z","iopub.execute_input":"2024-10-07T09:03:50.007007Z","iopub.status.idle":"2024-10-07T09:03:50.012655Z","shell.execute_reply.started":"2024-10-07T09:03:50.00698Z","shell.execute_reply":"2024-10-07T09:03:50.011656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ntrue_positives = 41\ntrue_negatives = 51\nfalse_negatives = 6\nfalse_positives = 2\n\nconfusion_mat = confusion_matrix([1, 0, 1, 0], [1, 0, 0, 1],\n                                  labels=[1, 0],\n                                  sample_weight=[true_positives, true_negatives, false_positives, false_negatives])\n\nlabels = ['Positive', 'Negative']\n\nsns.set()\nplt.figure(figsize=(7, 5))\nsns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:50.014159Z","iopub.execute_input":"2024-10-07T09:03:50.014829Z","iopub.status.idle":"2024-10-07T09:03:50.706678Z","shell.execute_reply.started":"2024-10-07T09:03:50.01479Z","shell.execute_reply":"2024-10-07T09:03:50.705533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'results' contains the training history\nplt.plot(results.history['loss'], label='Training Loss')\nplt.plot(results.history['Accuracy'], label='Training Accuracy')  # Change to 'accuracy' if that's the metric you're using\nplt.grid(True)\nplt.xlabel('Epochs')\nplt.ylabel('Value')\nplt.title('Training Loss and Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:50.707834Z","iopub.execute_input":"2024-10-07T09:03:50.708161Z","iopub.status.idle":"2024-10-07T09:03:50.90043Z","shell.execute_reply.started":"2024-10-07T09:03:50.708131Z","shell.execute_reply":"2024-10-07T09:03:50.899484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(results.history['val_loss'], label='Validation Loss')\nplt.plot(results.history['val_Accuracy'], label='Validation Accuracy')\nplt.grid(True)\nplt.xlabel('Epochs')\nplt.ylabel('Value')\nplt.title('Validation Loss and Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-07T09:03:50.901748Z","iopub.execute_input":"2024-10-07T09:03:50.902065Z","iopub.status.idle":"2024-10-07T09:03:51.43424Z","shell.execute_reply.started":"2024-10-07T09:03:50.902034Z","shell.execute_reply":"2024-10-07T09:03:51.433297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T11:03:15.646132Z","iopub.execute_input":"2024-10-07T11:03:15.647071Z","iopub.status.idle":"2024-10-07T11:03:21.982209Z","shell.execute_reply.started":"2024-10-07T11:03:15.646504Z","shell.execute_reply":"2024-10-07T11:03:21.980784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN for comparison","metadata":{}},{"cell_type":"code","source":"## import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nbatch_size = 100\nimg_height = 32\nimg_width = 32\nimg_shape = (img_height, img_width, 3)\n\ntraining_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/train',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size\n\n)\n\ntesting_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/test',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)\n\nvalidation_ds =  tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/accident-detection-from-cctv-footage/data/val',\n    seed=101,\n    image_size= (img_height, img_width),\n    batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:30:26.728837Z","iopub.execute_input":"2024-09-10T06:30:26.729314Z","iopub.status.idle":"2024-09-10T06:30:27.10091Z","shell.execute_reply.started":"2024-09-10T06:30:26.729267Z","shell.execute_reply":"2024-09-10T06:30:27.099848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,\n                                               include_top=False,\n                                               weights='imagenet')\n\nbase_model.trainable = False\n\nmodel = tf.keras.Sequential([\n    base_model,\n    layers.Conv2D(32, 3, activation='relu',padding=\"same\"),\n    layers.Conv2D(16, 3, activation='relu',padding=\"same\"),\n    layers.Flatten(),\n    layers.Dense(len(class_names), activation= 'softmax')\n])\ncbs = [\n    ModelCheckpoint(\"CNN-Model.h5\", save_best_only=True)\n  ]\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(training_ds, validation_data = validation_ds, epochs = 100,callbacks=cbs)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:30:28.040544Z","iopub.execute_input":"2024-09-10T06:30:28.040941Z","iopub.status.idle":"2024-09-10T06:33:54.443716Z","shell.execute_reply.started":"2024-09-10T06:30:28.040908Z","shell.execute_reply":"2024-09-10T06:33:54.442667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing CNN result with ViT","metadata":{}},{"cell_type":"code","source":"plt.plot(results.history['Accuracy'], label = 'Training Accuracy-ViT')\nplt.plot(history.history['accuracy'], label = 'Training Accuracy-CNN')\nplt.grid(True)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T06:34:13.920747Z","iopub.execute_input":"2024-09-10T06:34:13.921729Z","iopub.status.idle":"2024-09-10T06:34:14.173504Z","shell.execute_reply.started":"2024-09-10T06:34:13.921686Z","shell.execute_reply":"2024-09-10T06:34:14.172478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}